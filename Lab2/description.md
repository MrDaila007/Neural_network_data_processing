# Лабораторная работа №2
## Многослойный персептрон

### 1. Цель работы

Изучение топологии, алгоритма функционирования многослойного персептрона.

### 2. Теоретические сведения

Многослойный персептрон является сетью с прямым распространением сигнала (без обратных связей), обучаемой с учителем. Такая сеть способна аппроксимировать любую непрерывную функцию или границу между классами со сколь угодно высокой точностью. Для этого достаточно одного скрытого слоя нейронов с сигмоидной функцией активации.

#### Топология многослойного персептрона

Многослойный персептрон обычно состоит из 3 слоев:
- **Первый распределительный слой** (n входов)
- **Второй скрытый слой** (h нейронов)
- **Третий выходной слой** (m выходных нейронов)

```
                            v_ij              w_jk
    
    Входной слой      Скрытый слой       Выходной слой
    (распределит.)
    
         x₁ ───────────┬─────────────────┬────────────► y₁
          │            │                 │
          │    ┌───┐   │         ┌───┐   │    ┌───┐
          ├───►│ 1 │───┼────────►│ 1 │───┼───►│ 1 │───► y₁
          │    └───┘   │         └───┘   │    └───┘
          │            │                 │
         x₂ ──────┐    │                 │
          │       │    │         ┌───┐   │    ┌───┐
          │   ┌───┴───►│────────►│ 2 │───┼───►│ 2 │───► y₂
          ├───►       │         └───┘   │    └───┘
          │           │                  │
         ...         ...        ...     ...   ...
          │           │                  │
          │    ┌───┐   │         ┌───┐   │    ┌───┐
         xₙ ───►│ n │───┴────────►│ h │───┴───►│ m │───► yₘ
               └───┘             └───┘        └───┘
                                  ▲            ▲
                                  │            │
                                 Qⱼ           Tₖ
                              (пороги)     (пороги)
    
    Рис. 3.1. Топология многослойного персептрона
```

#### Параметры сети

Используются **две матрицы весов**:
- **Скрытого слоя** `V` размером n×h
- **Выходного слоя** `W` размером h×m

С каждым слоем нейронов связан **массив порогов**:
- `Q = (Q₁, Q₂, ..., Qₕ)` – для скрытого слоя
- `T = (T₁, T₂, ..., Tₘ)` – для выходного

Эти данные представляют собой **знания сети**, настраиваемые в процессе обучения и определяющие ее поведение.

#### Функционирование персептрона

Персептрон функционирует по следующим формулам:

**Для скрытого слоя:**
```
         ⎛  n              ⎞
gⱼ = f  ⎜  ∑ vᵢⱼ × xᵢ + Qⱼ ⎟
         ⎝ i=1             ⎠
```

**Для выходного слоя:**
```
         ⎛  h              ⎞
yₖ = f  ⎜  ∑ wⱼₖ × gⱼ + Tₖ ⎟
         ⎝ j=1             ⎠
```

В качестве функции активации используется **сигмоидная функция**:

```
            1
f(x) = ─────────────
       1 + e^(-x)
```

Вид функции определяет диапазон чисел, в котором работает сеть. Сигмоидная функция имеет область значений от 0 до 1.

#### Обучение с учителем

Обучение с учителем ставит перед сетью задачу обобщить p примеров, заданных парами векторов `(xʳ, yʳ)`, r = 1, p.

- **Вектор xʳ** = (x₁ʳ, x₂ʳ, ..., xₙʳ) — входной образ (вектор признаков)
- **Вектор yʳ** = (y₁ʳ, y₂ʳ, ..., yₘʳ) — эталонный выход, кодирующий номер класса

**Оптимальное кодирование классов:** номер класса определяется позицией единичной компоненты в векторе yʳ, а все остальные компоненты равны 0. Каждый выходной нейрон соответствует одному классу.

### Алгоритм обратного распространения ошибки

Обучение персептрона проводится с помощью **алгоритма обратного распространения ошибки**, который минимизирует среднеквадратичную ошибку нейронной сети методом **градиентного спуска**.

#### Этап 1. Инициализация

Присвоить всем весам и порогам случайные значения из диапазона [-1, 1].

#### Этап 2. Для каждой пары векторов (xʳ, yʳ):

**2.1. Прямой проход (вычисление выходов):**

Рассчитываются выходы нейронов скрытого слоя `gⱼ` и выходы сети `yₖ`.

**2.2. Обратный проход (коррекция весов):**

Определяем **взвешенную сумму** (аргумент функции активации):

```
      h
Sₖ = ∑ wⱼₖ × gⱼ + Tₖ
     j=1
```

**Ошибка k-го нейрона выходного слоя:**
```
dₖ = yₖʳ - yₖ
```

**Производная среднеквадратичной ошибки:**
```
∂E        ∂E     ∂yₖ    ∂Sₖ
───── = ────── × ───── × ─────
∂wⱼₖ     ∂yₖ     ∂Sₖ    ∂wⱼₖ
```

где:
- `∂E/∂yₖ = dₖ = yₖʳ - yₖ` — ошибка k-го нейрона
- `∂yₖ/∂Sₖ = f'(Sₖ)` — производная функции активации
- `∂Sₖ/∂wⱼₖ = gⱼ` — значение j-го нейрона предыдущего слоя

**Производная сигмоидной функции:**
```
f'(Sₖ) = f(Sₖ) × (1 - f(Sₖ)) = yₖ × (1 - yₖ)
```

**Ошибка j-го нейрона скрытого слоя** (обратное распространение):
```
      m
eⱼ = ∑ dₖ × f'(Sₖ) × wⱼₖ
     k=1
```

#### Формулы коррекции весов

**Для выходного слоя:**
```
wⱼₖ := wⱼₖ + α × yₖ × (1 - yₖ) × dₖ × gⱼ
Tₖ  := Tₖ  + α × yₖ × (1 - yₖ) × dₖ
```

**Для скрытого слоя:**
```
vᵢⱼ := vᵢⱼ + β × gⱼ × (1 - gⱼ) × eⱼ × xᵢ
Qⱼ  := Qⱼ  + β × gⱼ × (1 - gⱼ) × eⱼ
```

где:
- `α` — скорость обучения выходного слоя
- `β` — скорость обучения скрытого слоя

**Среднеквадратичная ошибка сети:**

```
      1   m
E = ───── ∑ (yₖʳ - yₖ)²
      2  k=1
```

#### Этап 3. Проверка условия завершения

**Условие прекращения обучения:**

```
max |dₖ| < D,  k = 1..m,  r = 1..p
```

где `D` – достаточно маленькая константа — величина максимальной ошибки, которую требуется достичь.

Если условие не выполняется, то этап 2 повторяется.

### Проблемы обучения

#### Оптимальное соотношение параметров

```
      p
h ~ ─────
      n
```

где:
- `h` – количество нейронов скрытого слоя
- `p` – количество примеров в обучающей выборке
- `n` – количество входов

#### Переобучение

Увеличение числа нейронов `h` скрытого слоя приводит к росту ошибки, связанной со **сложностью модели**. Персептрону легче провести функцию через эталонные точки, однако при этом обобщающая способность сети ухудшается — он хуже предсказывает поведение функции на образах, не входящих в обучающую выборку. Такое состояние сети называется **переобучением**.

#### Критические точки градиентного спуска

Алгоритм градиентного спуска не гарантирует нахождение **глобального минимума** среднеквадратичной ошибки — гарантируется определение только **локального минимума**.

```
       E (ошибка)
        │
        │    ╱╲
        │   ╱  ╲ B (локальный максимум)
        │  ╱    ╲      ╱╲
        │ ╱      ╲    ╱  ╲
        │╱    A   ╲  ╱ C  ╲     ╱
        │  (локал. ╲╱(точка ╲   ╱
        │  миним.)     перегиба)╲ ╱
        │                        ╲╱ D (глобальный
        │                           минимум)
        └────────────────────────────────────► W
    
    Рис. 3.2. Схематическая функция ошибки сети
```

| Точка | Тип | Описание |
|-------|-----|----------|
| A | Локальный минимум | Полное прекращение уменьшения ошибки. Может помочь повторное обучение с другим начальным распределением весов |
| B | Локальный максимум | Скорость резко падает, затем снова быстро растет |
| C | Точка перегиба | Характеризуется длительным уменьшением скорости |
| D | Глобальный минимум | Цель алгоритма |

#### Модификация с введением момента

Эффективной модификацией является введение **момента**, накапливающего влияние градиента на веса со временем:

```
                ∂E
Δw(t) = -α × ────── + μ × Δw(t-1)
               ∂w
```

где `μ` — параметр, определяющий величину влияния момента.

**Достоинства:**
- Большая скорость в точках перегиба
- Возможность по инерции преодолевать небольшие локальные минимумы

**Недостатки:**
- Еще один параметр, величину которого следует подбирать и настраивать

### 3. Задание

1. Ознакомьтесь с теоретической частью.
2. На языке С, С++ напишите программу, реализующую многослойный персептрон.
3. Произведите обучение многослойного персептрона. Исходные данные - 5 классов образов, размер идеального образа 6×6 (в соответствии с вариантом).
4. Подайте на вход сети ряд тестовых образов, по 3 зашумленных образа каждого из 5 классов.
5. Проанализируйте результаты работы программы, которые должны иметь следующий вид:
   - вывести распознаваемый зашумленный образ
   - вывести процент подобия распознаваемого зашумленного образа по отношению к каждому из 5 классов
   - вывести количество шагов, затраченных на обучение сети на заданное количество классов
6. Напишите отчет.

**Пример вывода результатов:**

```
┌─────────────────────────────────────────────────┐
│ Распознаваемый образ (6×6):                     │
│                                                 │
│   ■ ■ □ □ ■ ■                                   │
│   ■ □ ■ ■ □ ■                                   │
│   □ ■ ■ ■ ■ □                                   │
│   □ ■ ■ ■ ■ □                                   │
│   ■ □ ■ ■ □ ■                                   │
│   ■ ■ □ □ ■ ■                                   │
│                                                 │
├─────────────────────────────────────────────────┤
│ Процент подобия:                                │
│   Класс 1 (2): 12.3%                            │
│   Класс 2 (3): 8.7%                             │
│   Класс 3 (4): 5.2%                             │
│   Класс 4 (5): 3.1%                             │
│   Класс 5 (7): 92.4%  ◄── Распознан как "7"     │
│                                                 │
├─────────────────────────────────────────────────┤
│ Шагов обучения: 1542                            │
└─────────────────────────────────────────────────┘
```

#### Содержание отчета:

- топология многослойного персептрона;
- основные формулы обучения и воспроизведения;
- идеальные образы для обучения многослойного персептрона;
- тестовые зашумленные образы;
- результаты воспроизведения: процент подобия по отношению к каждому из классов, количество шагов, затраченных на обучение;
- результаты сравнения многослойного персептрона с нейронной сетью Хопфилда;
- выводы: преимущества и недостатки многослойного персептрона.

#### Варианты задания

| № варианта | 1-ый класс | 2-ой класс | 3-ий класс | 4-ый класс | 5-ый класс |
|------------|------------|------------|------------|------------|------------|
| 1          | 2          | 3          | 4          | 5          | 7          |
| 2          | N          | F          | I          | P          | D          |
| 3          | ∧          | ∨          | ∃          | ⊂          | ⊃          |
| 4          | ×          | ÷          | ×          | ÷          | ±          |
| 5          | ≤          | ≥          | ≠          | ≈          | ≅          |
| 6          | L          | U          | T          | O          | K          |
| 7          | →          | ←          | ↔          | ⇔          | ⇐          |

### 4. Контрольные вопросы

1. Топология многослойного персептрона.
2. Процесс обучения.
3. Процесс воспроизведения.
4. Процедура обратного распространения ошибки.
5. Каким образом можно улучшить качество распознавания?
6. Достоинства и недостатки данного типа нейронной сети.
