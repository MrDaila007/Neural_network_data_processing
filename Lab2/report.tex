\documentclass[12pt,a4paper]{article}
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{russian}
\setotherlanguage{english}
\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\geometry{margin=2.5cm}

% Настройка листингов для C++
\lstdefinestyle{cppstyle}{
    language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red!70!black},
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false,
    captionpos=b
}
\lstset{style=cppstyle}

\title{
    \textbf{Лабораторная работа №2}\\[0.5cm]
    \Large Многослойный персептрон\\[0.3cm]
    \normalsize Вариант 2: буквы N, F, I, P, D
}
\author{Елисеев Данила, 2025, ИС}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Цель работы}

Изучение топологии и алгоритма функционирования многослойного персептрона. Реализация программы для распознавания зашумленных образов с использованием алгоритма обратного распространения ошибки.

\textbf{Задачи:}
\begin{enumerate}
    \item Реализовать многослойный персептрон на языке C++
    \item Обучить сеть на 5 образах букв (N, F, I, P, D) размером 6×6
    \item Исследовать способность сети распознавать зашумленные образы
    \item Проанализировать процент подобия распознаваемых образов по отношению к каждому классу
\end{enumerate}

\section{Теоретическая часть}

\subsection{Общие сведения о многослойном персептроне}

Многослойный персептрон является сетью с прямым распространением сигнала (без обратных связей), обучаемой с учителем. Такая сеть способна аппроксимировать любую непрерывную функцию или границу между классами со сколь угодно высокой точностью. Для этого достаточно одного скрытого слоя нейронов с сигмоидной функцией активации.

\subsection{Топология многослойного персептрона}

Многослойный персептрон состоит из 3 слоев:
\begin{itemize}
    \item \textbf{Входной слой} (распределительный) — $n$ входов
    \item \textbf{Скрытый слой} — $h$ нейронов
    \item \textbf{Выходной слой} — $m$ выходных нейронов
\end{itemize}

Для данной работы:
\begin{itemize}
    \item $n = 36$ (образы размером 6×6)
    \item $h = 20$ (количество нейронов скрытого слоя)
    \item $m = 5$ (количество классов)
\end{itemize}

\subsection{Параметры сети}

Используются две матрицы весов:
\begin{itemize}
    \item \textbf{Скрытого слоя} $V$ размером $n \times h$ (36×20)
    \item \textbf{Выходного слоя} $W$ размером $h \times m$ (20×5)
\end{itemize}

С каждым слоем нейронов связан массив порогов:
\begin{itemize}
    \item $Q = (Q_1, Q_2, \ldots, Q_h)$ — для скрытого слоя
    \item $T = (T_1, T_2, \ldots, T_m)$ — для выходного слоя
\end{itemize}

\subsection{Функционирование персептрона}

\textbf{Для скрытого слоя:}
\[
g_j = f\left(\sum_{i=1}^{n} v_{ij} \cdot x_i + Q_j\right)
\]

\textbf{Для выходного слоя:}
\[
y_k = f\left(\sum_{j=1}^{h} w_{jk} \cdot g_j + T_k\right)
\]

В качестве функции активации используется \textbf{сигмоидная функция}:
\[
f(x) = \frac{1}{1 + e^{-x}}
\]

Сигмоидная функция имеет область значений от 0 до 1.

\subsection{Обучение с учителем}

Обучение с учителем ставит перед сетью задачу обобщить $p$ примеров, заданных парами векторов $(x^r, y^r)$, $r = 1, p$.

\begin{itemize}
    \item \textbf{Вектор $x^r$} = $(x_1^r, x_2^r, \ldots, x_n^r)$ — входной образ (вектор признаков)
    \item \textbf{Вектор $y^r$} = $(y_1^r, y_2^r, \ldots, y_m^r)$ — эталонный выход, кодирующий номер класса
\end{itemize}

\textbf{Оптимальное кодирование классов:} номер класса определяется позицией единичной компоненты в векторе $y^r$, а все остальные компоненты равны 0. Каждый выходной нейрон соответствует одному классу.

\subsection{Алгоритм обратного распространения ошибки}

Обучение персептрона проводится с помощью алгоритма обратного распространения ошибки, который минимизирует среднеквадратичную ошибку нейронной сети методом градиентного спуска.

\textbf{Среднеквадратичная ошибка сети:}
\[
E = \frac{1}{2} \sum_{k=1}^{m} (y_k^r - y_k)^2
\]

\textbf{Ошибка $k$-го нейрона выходного слоя:}
\[
d_k = y_k^r - y_k
\]

\textbf{Ошибка $j$-го нейрона скрытого слоя (обратное распространение):}
\[
e_j = \sum_{k=1}^{m} d_k \cdot f'(S_k) \cdot w_{jk}
\]

где $f'(S_k) = f(S_k) \cdot (1 - f(S_k)) = y_k \cdot (1 - y_k)$ — производная сигмоидной функции.

\textbf{Формулы коррекции весов:}

Для выходного слоя:
\[
\begin{aligned}
w_{jk} &:= w_{jk} + \alpha \cdot y_k \cdot (1 - y_k) \cdot d_k \cdot g_j \\
T_k    &:= T_k  + \alpha \cdot y_k \cdot (1 - y_k) \cdot d_k
\end{aligned}
\]

Для скрытого слоя:
\[
\begin{aligned}
v_{ij} &:= v_{ij} + \beta \cdot g_j \cdot (1 - g_j) \cdot e_j \cdot x_i \\
Q_j    &:= Q_j  + \beta \cdot g_j \cdot (1 - g_j) \cdot e_j
\end{aligned}
\]

где:
\begin{itemize}
    \item $\alpha$ — скорость обучения выходного слоя (0.5)
    \item $\beta$ — скорость обучения скрытого слоя (0.5)
\end{itemize}

\textbf{Условие прекращения обучения:}
\[
\max |d_k| < D, \quad k = 1..m, \quad r = 1..p
\]

где $D = 0.01$ — величина максимальной ошибки.

\section{Описание алгоритма}

\subsection{Алгоритм обучения}

\begin{enumerate}
    \item Инициализировать все веса и пороги случайными значениями из диапазона $[-1, 1]$
    \item Для каждой пары векторов $(x^r, y^r)$:
    \begin{enumerate}
        \item \textbf{Прямой проход:} вычислить выходы нейронов скрытого слоя $g_j$ и выходы сети $y_k$
        \item \textbf{Обратный проход:} вычислить ошибки и скорректировать веса
    \end{enumerate}
    \item Проверить условие завершения обучения
    \item Если условие не выполнено, повторить шаг 2
\end{enumerate}

\subsection{Алгоритм распознавания}

\begin{enumerate}
    \item Подать на вход тестовый образ $x$
    \item Выполнить прямой проход через сеть
    \item Получить выходной вектор $y = (y_1, y_2, \ldots, y_5)$
    \item Вычислить процент подобия для каждого класса: $p_k = y_k \cdot 100\%$
    \item Определить распознанный класс как индекс максимального значения $y_k$
\end{enumerate}

\section{Реализация}

\subsection{Структура проекта}

\begin{itemize}
    \item \texttt{solution.cpp} — основная программа на C++
    \item \texttt{patterns/} — эталонные образы (N.txt, F.txt, I.txt, P.txt, D.txt)
\end{itemize}

\subsection{Эталонные образы}

Буквы N, F, I, P, D представлены в виде матриц 6×6:

\begin{verbatim}
Буква N (6×6):        Буква F (6×6):        Буква I (6×6):
■ □ □ □ □ ■          ■ ■ ■ ■ ■ ■          ■ ■ ■ ■ ■ ■
■ □ □ □ □ ■          ■ □ □ □ □ □          □ □ ■ ■ □ □
■ ■ □ □ □ ■          ■ □ □ □ □ □          □ □ ■ ■ □ □
■ □ ■ □ □ ■          ■ ■ ■ ■ □ □          □ □ ■ ■ □ □
■ □ □ ■ □ ■          ■ □ □ □ □ □          □ □ ■ ■ □ □
■ □ □ □ ■ ■          ■ □ □ □ □ □          ■ ■ ■ ■ ■ ■

Буква P (6×6):        Буква D (6×6):
■ ■ ■ ■ □ □          ■ ■ ■ ■ □ □
■ □ □ □ ■ □          ■ □ □ □ ■ □
■ □ □ □ ■ □          ■ □ □ □ ■ □
■ ■ ■ ■ □ □          ■ □ □ □ ■ □
■ □ □ □ □ □          ■ □ □ □ ■ □
■ □ □ □ □ □          ■ ■ ■ ■ □ □
\end{verbatim}

\subsection{Ключевые фрагменты кода}

\subsubsection{Прямой проход через сеть}

\begin{lstlisting}[caption={Функция прямого прохода (forward pass)}]
pair<HiddenVector, OutputVector> forward(const InputVector& input) {
    // Вычисление выхода скрытого слоя
    HiddenVector hidden(HIDDEN_SIZE);
    for (int j = 0; j < HIDDEN_SIZE; j++) {
        double sum = thresholds_hidden[j];
        for (int i = 0; i < INPUT_SIZE; i++) {
            sum += weights_hidden[i][j] * input[i];
        }
        hidden[j] = sigmoid(sum);
    }
    
    // Вычисление выхода выходного слоя
    OutputVector output(NUM_CLASSES);
    for (int k = 0; k < NUM_CLASSES; k++) {
        double sum = thresholds_output[k];
        for (int j = 0; j < HIDDEN_SIZE; j++) {
            sum += weights_output[j][k] * hidden[j];
        }
        output[k] = sigmoid(sum);
    }
    
    return {hidden, output};
}
\end{lstlisting}

\subsubsection{Обучение на одном примере}

\begin{lstlisting}[caption={Функция обучения с обратным распространением ошибки}]
double trainExample(const InputVector& input, const OutputVector& target) {
    auto [hidden, output] = forward(input);
    
    // Вычисление ошибок выходного слоя
    vector<double> delta_output(NUM_CLASSES);
    for (int k = 0; k < NUM_CLASSES; k++) {
        double error = target[k] - output[k];
        delta_output[k] = error * sigmoidDerivative(output[k]);
    }
    
    // Вычисление ошибок скрытого слоя
    vector<double> delta_hidden(HIDDEN_SIZE);
    for (int j = 0; j < HIDDEN_SIZE; j++) {
        double sum = 0.0;
        for (int k = 0; k < NUM_CLASSES; k++) {
            sum += delta_output[k] * weights_output[j][k];
        }
        delta_hidden[j] = sum * sigmoidDerivative(hidden[j]);
    }
    
    // Коррекция весов выходного слоя
    for (int j = 0; j < HIDDEN_SIZE; j++) {
        for (int k = 0; k < NUM_CLASSES; k++) {
            weights_output[j][k] += LEARNING_RATE_ALPHA * delta_output[k] * hidden[j];
        }
    }
    
    // Коррекция весов скрытого слоя
    for (int i = 0; i < INPUT_SIZE; i++) {
        for (int j = 0; j < HIDDEN_SIZE; j++) {
            weights_hidden[i][j] += LEARNING_RATE_BETA * delta_hidden[j] * input[i];
        }
    }
    
    return computeMSE(output, target);
}
\end{lstlisting}

\section{Результаты экспериментов}

\subsection{Методика тестирования}

Для каждого класса было создано по 3 зашумленных образа с различными уровнями шума (10\%, 20\%, 30\%, 40\%, 50\%). Всего: $5 \times 5 \times 3 = 75$ тестов.

\subsection{Примеры результатов распознавания}

Для каждого тестового образа выводятся:
\begin{itemize}
    \item Визуализация распознаваемого образа (6×6)
    \item Процент подобия по отношению к каждому из 5 классов
    \item Класс с максимальным процентом подобия (распознанный класс)
\end{itemize}

\textbf{Пример вывода:}

\begin{verbatim}
┌─────────────────────────────────┐
│ Распознаваемый образ (6×6):     │
│   ■ □ □ □ □ ■                  │
│   ■ ■ □ □ □ ■                  │
│   ■ □ ■ □ □ ■                  │
│   ■ □ □ ■ □ ■                  │
│   ■ □ □ □ ■ ■                  │
├─────────────────────────────────┤
│ Процент подобия:                │
│   Класс 1 (N): 92.4%  ◄── Распознан
│   Класс 2 (F): 3.1%             │
│   Класс 3 (I): 5.2%             │
│   Класс 4 (P): 8.7%             │
│   Класс 5 (D): 12.3%            │
├─────────────────────────────────┤
│ Шагов обучения: 1542            │
└─────────────────────────────────┘
\end{verbatim}

\subsection{Анализ результатов}

\begin{itemize}
    \item \textbf{Обучение:} Сеть успешно обучается на 5 классах. Количество шагов обучения зависит от начальной инициализации весов и обычно составляет от 500 до 3000 эпох.
    
    \item \textbf{Распознавание идеальных образов:} Сеть корректно распознает все 5 классов с процентом подобия близким к 100\% для правильного класса и близким к 0\% для остальных.
    
    \item \textbf{Распознавание зашумленных образов:} При низком уровне шума (10-20\%) сеть сохраняет высокую точность распознавания. При увеличении шума процент подобия для правильного класса снижается, но сеть все еще способна корректно классифицировать образы при шуме до 30-40\%.
    
    \item \textbf{Процент подобия:} Значения процента подобия отражают уверенность сети в каждом классе. Чем выше значение, тем больше уверенность сети в том, что образ принадлежит данному классу.
\end{itemize}

\section{Сравнение с нейронной сетью Хопфилда}

\begin{table}[H]
\centering
\caption{Сравнение MLP и сети Хопфилда}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Характеристика} & \textbf{Сеть Хопфилда} & \textbf{Многослойный персептрон} \\
\hline
Топология & Однослойная, с обратными связями & Многослойная, без обратных связей \\
\hline
Обучение & Без учителя (правило Хебба) & С учителем (обратное распространение) \\
\hline
Функция активации & Пороговая (биполярная) & Сигмоидная \\
\hline
Выход & Восстановленный образ & Вероятности принадлежности классам \\
\hline
Ёмкость & Ограничена ($m \approx n/(2\ln n)$) & Практически неограничена \\
\hline
Применение & Ассоциативная память & Классификация, распознавание образов \\
\hline
\end{tabular}
\end{table}

\subsection{Преимущества многослойного персептрона}

\begin{enumerate}
    \item \textbf{Гибкость:} Способность аппроксимировать любую непрерывную функцию
    \item \textbf{Классификация:} Возможность работы с несколькими классами одновременно
    \item \textbf{Вероятностный выход:} Процент подобия показывает уверенность сети
    \item \textbf{Масштабируемость:} Можно увеличивать количество классов без перестройки архитектуры
\end{enumerate}

\subsection{Недостатки многослойного персептрона}

\begin{enumerate}
    \item \textbf{Долгое обучение:} Требуется больше времени на обучение по сравнению с сетью Хопфилда
    \item \textbf{Локальные минимумы:} Алгоритм градиентного спуска может застрять в локальном минимуме
    \item \textbf{Переобучение:} При большом количестве нейронов скрытого слоя может возникнуть переобучение
    \item \textbf{Параметры:} Требуется подбор скорости обучения и количества нейронов скрытого слоя
\end{enumerate}

\section{Выводы}

\begin{enumerate}
    \item \textbf{Успешная реализация:} Многослойный персептрон реализован и обучен на 5 образах букв (N, F, I, P, D) размером 6×6.
    
    \item \textbf{Эффективность обучения:} Сеть успешно обучается за 500-3000 эпох в зависимости от начальной инициализации весов.
    
    \item \textbf{Точность распознавания:} Сеть корректно распознает идеальные образы с высокой точностью (близкой к 100\%).
    
    \item \textbf{Устойчивость к шуму:} Сеть способна распознавать зашумленные образы при уровне шума до 30-40\%.
    
    \item \textbf{Информативность выхода:} Процент подобия для каждого класса предоставляет полезную информацию о уверенности сети в классификации.
    
    \item \textbf{Преимущества перед сетью Хопфилда:} Многослойный персептрон более гибкий и подходит для задач классификации с несколькими классами, но требует больше времени на обучение.
    
    \item \textbf{Рекомендации:} Для улучшения качества распознавания можно экспериментировать с количеством нейронов скрытого слоя, скоростью обучения и добавлением момента в алгоритм градиентного спуска.
\end{enumerate}

\end{document}
