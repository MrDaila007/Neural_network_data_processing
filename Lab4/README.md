# Лабораторная работа №4: Конкурентная нейронная сеть

## Цель работы

Изучение топологии и алгоритма функционирования конкурентной нейронной сети. Реализация программы для кластеризации и распознавания образов математических символов.

## Теоретические сведения

Конкурентная нейронная сеть — простейшая самоорганизующаяся нейронная сеть, которая обучается **без учителя**. Используется для кластеризации, сжатия данных и выделения признаков.

### Архитектура сети

```
    Входной слой           Выходной слой
    (36 нейронов)          (5 нейронов)
    
         x₁ ──────────────────┐
          │       w₁ⱼ         │
          │    ┌──────────────┼───────► y₁
          │    │              │         ↑
          │    │              │         │
         x₂ ───┼──────────────┼───────► y₂    "Победитель
          │    │    w₂ⱼ       │         ↑      получает
          │    │              │         │      всё"
         ...  ...            ...       ...      (WTA)
          │    │              │         │
          │    │              │         │
         x₃₆───┴──────────────┴───────► y₅
                                        ↑
                              Только один нейрон
                              активен (победитель)
```

### Основные формулы

**Выход нейрона (для нормированных векторов):**
```
y_j = Σ(i=1 to n) w_ij × x_i = cos(α)
```

где α — угол между входным вектором и вектором весов.

**Обучение (правило WTA — Winner Takes All):**
```
w_ij(t+1) = w_ij(t) + β × (x_i - w_ij(t))
```

**Нормировка весов:**
```
w_ij(t+1) = w_ij(t+1) / ||w_j(t+1)||
```

**Частотно-зависимое обучение:**
```
winner = argmin_j (||x - w_j|| × f_j)
```

где f_j — количество побед j-го нейрона.

## Вариант задания

**Вариант 5:** Символы **≤**, **≥**, **≠**, **≈**, **≅** (размер 6×6)

### Эталонные образы

```
Символ ≤ (LE):        Символ ≥ (GE):        Символ ≠ (NE):
□ □ □ □ □ □          □ □ □ □ □ □          ■ ■ □ □ ■ ■
□ □ □ □ □ □          □ □ □ □ □ □          ■ ■ □ □ ■ ■
□ □ ■ ■ ■ □          ■ ■ ■ ■ ■ ■          □ □ □ □ □ □
□ □ □ □ □ □          □ □ □ □ □ □          □ □ □ □ □ □
■ ■ ■ ■ ■ ■          □ □ ■ ■ ■ □          ■ ■ □ □ ■ ■
□ □ □ □ □ □          □ □ □ □ □ □          ■ ■ □ □ ■ ■

Символ ≈ (AP):        Символ ≅ (CO):
□ □ □ □ □ □          □ □ □ □ □ □
□ ■ ■ ■ ■ □          □ ■ ■ ■ ■ □
■ □ □ □ □ ■          ■ □ □ □ □ ■
□ □ □ □ □ □          □ □ ■ ■ □ □
■ □ □ □ □ ■          ■ □ □ □ □ ■
□ ■ ■ ■ ■ □          □ ■ ■ ■ ■ □
```

## Сборка и запуск

### Требования

- Компилятор C++ с поддержкой C++17 (g++ 8+, clang++ 7+)
- Стандартная библиотека C++ (filesystem)

### Компиляция

```bash
cd Lab4
g++ -std=c++17 solution.cpp -o solution
```

### Запуск

```bash
./solution
```

## Структура файлов

```
Lab4/
├── README.md           # Этот файл
├── description.md      # Подробное описание задания
├── solution.cpp        # Исходный код решения
├── solution            # Скомпилированная программа
├── report.tex          # Отчет в формате LaTeX
├── report.pdf          # Скомпилированный отчет
├── patterns/           # Эталонные образы
│   ├── LE.txt          # Символ ≤
│   ├── GE.txt          # Символ ≥
│   ├── NE.txt          # Символ ≠
│   ├── AP.txt          # Символ ≈
│   └── CO.txt          # Символ ≅
└── tests/              # Тестовые образы с шумом
    ├── LE/
    ├── GE/
    ├── NE/
    ├── AP/
    └── CO/
```

## Пример вывода программы

```
========================================================
    ЛАБОРАТОРНАЯ РАБОТА №4: КОНКУРЕНТНАЯ СЕТЬ
    Вариант 5: символы ≤, ≥, ≠, ≈, ≅
========================================================

1. Загрузка эталонных образов из patterns/...
   Загружено 15 обучающих образов

Эталонные образы (6x6):
------------------------
Класс ≤ (LE):
  □ □ □ □ □ □ 
  □ □ □ □ □ □ 
  □ □ ■ ■ ■ □ 
  □ □ □ □ □ □ 
  ■ ■ ■ ■ ■ ■ 
  □ □ □ □ □ □ 

2. Инициализация весов случайными значениями...
   Матрица весов инициализирована (5 нейронов x 36 входов)

3. Обучение сети...
   Обучение завершено на итерации 42 (макс. расстояние: 0.0089)

4. Определение соответствия нейронов классам:
   Класс ≤ → Нейрон-победитель: #0
   Класс ≥ → Нейрон-победитель: #1
   Класс ≠ → Нейрон-победитель: #2
   Класс ≈ → Нейрон-победитель: #3
   Класс ≅ → Нейрон-победитель: #4

5. Демонстрация распознавания (30% шума):
   Зашумленный образ класса ≠:
   Результат: Нейрон-победитель #2 (соответствует классу ≠) ✓

6. Статистика кластеризации:
   Нейрон #0: 3 образа (основной класс: ≤)
   Нейрон #1: 3 образа (основной класс: ≥)
   Нейрон #2: 3 образа (основной класс: ≠)
   Нейрон #3: 3 образа (основной класс: ≈)
   Нейрон #4: 3 образа (основной класс: ≅)
```

## Параметры сети

| Параметр | Значение |
|----------|----------|
| Входной слой | 36 нейронов |
| Выходной слой | 5 нейронов |
| Обучающих образов | 15 (по 3 на класс) |
| Скорость обучения β | 0.1 |
| Макс. расстояние | 0.01 |

## Результаты

| Уровень шума | Правильное распознавание |
|--------------|--------------------------|
| 10%          | ~100%                    |
| 20%          | ~90%                     |
| 30%          | ~80%                     |
| 40%          | ~64%                     |
| 50%          | ~52%                     |

## Сравнение нейронных сетей

| Характеристика | MLP | Сеть РБФ | Конкурентная |
|----------------|-----|----------|--------------|
| Тип обучения | С учителем | С учителем | Без учителя |
| Скорость обучения | Медленная | Быстрая | Быстрая |
| Назначение | Классификация | Классификация | Кластеризация |
| Необходимость эталонов | Да | Да | Нет |
| Интерпретируемость | Средняя | Средняя | Высокая |

## Преимущества

- **Обучение без учителя:** не требуются размеченные данные
- **Быстрое обучение:** сходится за ~50-100 итераций
- **Интерпретируемость:** понятная кластеризация
- **Простота:** легко реализовать и понять

## Недостатки

- **Требует знания количества кластеров:** нужно заранее задать число нейронов
- **Проблема "мёртвых нейронов":** решается частотно-зависимым обучением
- **Чувствительность к инициализации:** начальные веса влияют на результат
- **Меньшая точность:** уступает обучению с учителем

## Выводы

1. Конкурентная сеть успешно кластеризует образы без учителя
2. Частотно-зависимое обучение решает проблему "мёртвых нейронов"
3. Похожие образы проецируются в один кластер
4. При низком шуме (10-20%) достигается высокая точность
5. Рекомендуется для кластеризации данных без предварительной разметки

## Компиляция отчета

```bash
cd Lab4
xelatex report.tex
```

Требуется XeLaTeX и шрифты DejaVu.

