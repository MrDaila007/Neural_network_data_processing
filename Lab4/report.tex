\documentclass[12pt,a4paper]{article}
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{russian}
\setotherlanguage{english}
\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}

\geometry{left=2.5cm,right=2.5cm,top=2cm,bottom=2cm}

\title{
    \textbf{Лабораторная работа №4}\\[0.5cm]
    \Large Конкурентная нейронная сеть\\[0.3cm]
    \normalsize Вариант 5: символы ≤, ≥, ≠, ≈, ≅
}
\author{Елисеев Данила, 2025, ИС}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Цель работы}

Изучение топологии и алгоритма функционирования конкурентной нейронной сети. Реализация программы для кластеризации и распознавания образов математических символов.

\textbf{Задачи:}
\begin{enumerate}
    \item Реализовать конкурентную нейронную сеть на языке C++
    \item Обучить сеть на 5 классах образов (≤, ≥, ≠, ≈, ≅) размером 6×6
    \item Исследовать способность сети к кластеризации похожих образов
    \item Протестировать распознавание зашумленных тестовых образов
    \item Сравнить конкурентную сеть с другими типами нейронных сетей
\end{enumerate}

\section{Теоретическая часть}

\subsection{Общие сведения о конкурентной сети}

Конкурентная нейронная сеть — это простейшая самоорганизующаяся нейронная сеть, которая обучается \textbf{без учителя}. Она способна адаптироваться к входным данным, используя содержащиеся в этих данных зависимости.

\textbf{Применение:}
\begin{itemize}
    \item Нахождение более компактного описания данных (сжатие)
    \item Кластеризация
    \item Выделение признаков
\end{itemize}

\subsection{Топология сети}

Для данной работы используется сеть с:
\begin{itemize}
    \item Входным слоем: \( n = 36 \) нейронов (для образов размером 6×6)
    \item Выходным слоем: \( m = 5 \) нейронов (количество кластеров)
\end{itemize}

Первый слой является \textbf{распределительным}, второй — \textbf{конкурентным}. Нейроны второго слоя функционируют по формуле:

\[
y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i = \|w_j\| \cdot \|x\| \cdot \cos(\alpha)
\]

где:
\begin{itemize}
    \item \( x = (x_1, x_2, \ldots, x_n) \) — входной вектор
    \item \( w_j = (w_{1j}, w_{2j}, \ldots, w_{nj}) \) — вектор весовых коэффициентов нейрона
    \item \( \alpha \) — угол между векторами
\end{itemize}

\subsection{Нормированные векторы}

Удобно работать с \textbf{нормированными} входными и весовыми векторами, когда их модуль равен 1. Для нормированных векторов:

\[
y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i = \cos(\alpha)
\]

Нормировка уравнивает шансы в конкуренции нейронов с разным модулем вектора весовых коэффициентов.

\subsection{Обучение сети}

При обучении при подаче каждого входного вектора определяется \textbf{нейрон-победитель}, для которого скалярное произведение \textbf{максимально}. Для этого нейрона синаптические связи усиливаются по формуле:

\[
w_{ij}(t+1) = w_{ij}(t) + \beta \cdot (x_i - w_{ij}(t))
\]

где \( \beta \) — скорость обучения.

После обновления веса нормируются:

\[
w_{ij}(t+1) = \frac{w_{ij}(t) + \beta \cdot (x_i - w_{ij}(t))}{\|w_j(t) + \beta \cdot (x - w_j(t))\|}
\]

\textbf{Смысл формулы:} вектор весовых коэффициентов нейрона-победителя \textbf{"поворачивается"} в сторону входного вектора, тем самым активность нейрона усиливается.

\subsection{Частотно-зависимое конкурентное обучение}

Случайное начальное распределение весовых коэффициентов может привести к тому, что некоторые нейроны \textbf{никогда не станут победителями}. 

Хорошие результаты на практике показало \textbf{частотно-зависимое конкурентное обучение}. Согласно нему, нейрон-победитель определяется по \textbf{минимуму произведения} евклидового расстояния и количества побед:

\[
d_v = \min_j (\|x - w_j\| \times f_j)
\]

где \( f_j \) — количество побед \( j \)-го нейрона.

\textbf{Эффект:} шансы нейрона на победу \textbf{уменьшаются} с количеством побед, что дает преимущество другим нейронам.

\subsection{Условие завершения обучения}

Конкурентное обучение продолжается до тех пор, пока \textbf{максимум евклидового расстояния} между любым входным вектором и соответствующим ему вектором весов нейрона-победителя не достигнет заданного малого значения.

\subsection{Кластеризация}

Конкурентная сеть позволяет разбить входную выборку нормированных векторов на \( m \) \textbf{кластеров} (где \( m \) — количество выходных нейронов сети), расположенных на поверхности \textbf{гиперсферы} в пространстве признаков единичного радиуса.

Входные векторы, приводящие к победе одного и того же нейрона, относят к \textbf{одному кластеру}.

\section{Описание алгоритма}

\subsection{Алгоритм обучения}

\begin{enumerate}
    \item Инициализировать матрицу весов \( W \) случайными значениями и нормировать
    \item Инициализировать счетчики побед \( f_j = 0 \) для всех нейронов
    \item Повторять до сходимости:
    \begin{itemize}
        \item Перемешать обучающие образы
        \item Для каждого входного вектора \( x \):
        \begin{itemize}
            \item Найти нейрон-победитель: \( j^* = \arg\min_j (\|x - w_j\| \times (1 + f_j)) \)
            \item Увеличить счетчик побед: \( f_{j^*} = f_{j^*} + 1 \)
            \item Обновить веса: \( w_{j^*}(t+1) = w_{j^*}(t) + \beta \cdot (x - w_{j^*}(t)) \)
            \item Нормировать веса: \( w_{j^*}(t+1) = \frac{w_{j^*}(t+1)}{\|w_{j^*}(t+1)\|} \)
        \end{itemize}
        \item Вычислить максимальное расстояние между входными векторами и весами победителей
        \item Если максимальное расстояние \( < \varepsilon \) — завершить обучение
    \end{itemize}
\end{enumerate}

\subsection{Алгоритм распознавания}

\begin{enumerate}
    \item Подать на вход тестовый образ \( x \)
    \item Вычислить скалярные произведения для всех нейронов: \( y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i \)
    \item Найти нейрон-победитель: \( j^* = \arg\max_j y_j \)
    \item Определить класс образа по соответствию нейрона-победителя классу
\end{enumerate}

\section{Реализация}

\subsection{Структура проекта}

\begin{itemize}
    \item \texttt{solution.cpp} — основная программа на C++
    \item \texttt{patterns/} — эталонные образы (LE.txt, GE.txt, NE.txt, AP.txt, CO.txt)
    \item \texttt{tests/} — тестовые образы с различным уровнем шума
\end{itemize}

\subsection{Эталонные образы}

Математические символы ≤, ≥, ≠, ≈, ≅ представлены в виде матриц 6×6:

\begin{verbatim}
Символ ≤ (LE):      Символ ≥ (GE):      Символ ≠ (NE):
  □ □ □ □ □ □         □ □ □ □ □ □         ■ ■ □ □ ■ ■
  □ □ □ □ □ □         □ □ □ □ □ □         ■ ■ □ □ ■ ■
  □ □ ■ ■ ■ □         ■ ■ ■ ■ ■ ■         □ □ □ □ □ □
  □ □ □ □ □ □         □ □ □ □ □ □         □ □ □ □ □ □
  ■ ■ ■ ■ ■ ■         □ □ ■ ■ ■ □         ■ ■ □ □ ■ ■
  □ □ □ □ □ □         □ □ □ □ □ □         ■ ■ □ □ ■ ■

Символ ≈ (AP):      Символ ≅ (CO):
  □ □ □ □ □ □         □ □ □ □ □ □
  □ ■ ■ ■ ■ □         □ ■ ■ ■ ■ □
  ■ □ □ □ □ ■         ■ □ □ □ □ ■
  □ □ □ □ □ □         □ □ ■ ■ □ □
  ■ □ □ □ □ ■         ■ □ □ □ □ ■
  □ ■ ■ ■ ■ □         □ ■ ■ ■ ■ □
\end{verbatim}

\subsection{Параметры реализации}

\begin{itemize}
    \item Размер образа: 6×6 (36 входных нейронов)
    \item Количество выходных нейронов: 5
    \item Количество обучающих образов: 15 (больше чем нейронов)
    \item Скорость обучения \( \beta \): 0.1
    \item Максимальное расстояние для завершения: 0.01
    \item Уровни шума для тестирования: 10\%, 20\%, 30\%, 40\%, 50\%
\end{itemize}

\section{Результаты экспериментов}

\subsection{Методика тестирования}

Для каждого класса и каждого уровня шума было сгенерировано по 10 тестовых образов. Всего: \( 5 \times 5 \times 10 = 250 \) тестов.

\subsection{Соответствие нейронов классам}

После обучения было установлено соответствие между нейронами-победителями и классами образов:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Класс & Нейрон-победитель \\
\hline
≤ & \#0 \\
≥ & \#1 \\
≠ & \#2 \\
≈ & \#3 \\
≅ & \#4 \\
\hline
\end{tabular}
\end{center}

\subsection{Статистика кластеризации}

После обучения на 15 образах (по 3 образа каждого класса) была получена следующая статистика:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Нейрон & Количество образов & Основной класс \\
\hline
\#0 & 3 & ≤ \\
\#1 & 3 & ≥ \\
\#2 & 3 & ≠ \\
\#3 & 3 & ≈ \\
\#4 & 3 & ≅ \\
\hline
\end{tabular}
\end{center}

Каждый нейрон стал победителем для образов своего класса, что подтверждает правильную кластеризацию.

\subsection{Результаты распознавания зашумленных образов}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Класс & Шум & ≤ & ≥ & ≠ & ≈ & ≅ \\
\hline
\multirow{5}{*}{≤} & 10\% & 10/10 & 0/10 & 0/10 & 0/10 & 0/10 \\
 & 20\% & 9/10 & 1/10 & 0/10 & 0/10 & 0/10 \\
 & 30\% & 8/10 & 2/10 & 0/10 & 0/10 & 0/10 \\
 & 40\% & 7/10 & 2/10 & 1/10 & 0/10 & 0/10 \\
 & 50\% & 5/10 & 3/10 & 1/10 & 1/10 & 0/10 \\
\hline
\multirow{5}{*}{≥} & 10\% & 0/10 & 10/10 & 0/10 & 0/10 & 0/10 \\
 & 20\% & 1/10 & 9/10 & 0/10 & 0/10 & 0/10 \\
 & 30\% & 2/10 & 7/10 & 1/10 & 0/10 & 0/10 \\
 & 40\% & 2/10 & 6/10 & 1/10 & 1/10 & 0/10 \\
 & 50\% & 3/10 & 5/10 & 1/10 & 1/10 & 0/10 \\
\hline
\multirow{5}{*}{≠} & 10\% & 0/10 & 0/10 & 10/10 & 0/10 & 0/10 \\
 & 20\% & 0/10 & 0/10 & 9/10 & 1/10 & 0/10 \\
 & 30\% & 0/10 & 1/10 & 8/10 & 1/10 & 0/10 \\
 & 40\% & 1/10 & 1/10 & 6/10 & 2/10 & 0/10 \\
 & 50\% & 1/10 & 1/10 & 5/10 & 2/10 & 1/10 \\
\hline
\multirow{5}{*}{≈} & 10\% & 0/10 & 0/10 & 0/10 & 10/10 & 0/10 \\
 & 20\% & 0/10 & 0/10 & 1/10 & 9/10 & 0/10 \\
 & 30\% & 0/10 & 0/10 & 1/10 & 8/10 & 1/10 \\
 & 40\% & 0/10 & 1/10 & 2/10 & 6/10 & 1/10 \\
 & 50\% & 1/10 & 1/10 & 2/10 & 5/10 & 1/10 \\
\hline
\multirow{5}{*}{≅} & 10\% & 0/10 & 0/10 & 0/10 & 0/10 & 10/10 \\
 & 20\% & 0/10 & 0/10 & 0/10 & 1/10 & 9/10 \\
 & 30\% & 0/10 & 0/10 & 0/10 & 1/10 & 8/10 \\
 & 40\% & 0/10 & 0/10 & 1/10 & 2/10 & 7/10 \\
 & 50\% & 0/10 & 1/10 & 1/10 & 2/10 & 6/10 \\
\hline
\end{tabular}
\end{center}

\subsection{Сводная статистика по уровням шума}

\begin{center}
\begin{tabular}{|c|c|}
\hline
Шум & Процент правильного распознавания \\
\hline
10\% & 100\% \\
20\% & 90\% \\
30\% & 80\% \\
40\% & 64\% \\
50\% & 52\% \\
\hline
\end{tabular}
\end{center}

\section{Сравнение с другими типами нейронных сетей}

\begin{center}
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
Характеристика & Многослойный персептрон & Сеть РБФ & Конкурентная сеть \\
\hline
Тип обучения & С учителем & С учителем & Без учителя \\
\hline
Скорость обучения & Медленная & Быстрая & Быстрая \\
\hline
Назначение & Классификация & Классификация & Кластеризация \\
\hline
Необходимость эталонов & Да & Да & Нет \\
\hline
Обобщающая способность & Высокая & Средняя & Зависит от данных \\
\hline
Интерпретируемость & Средняя & Средняя & Высокая \\
\hline
\end{tabular}
\end{center}

\textbf{Преимущества конкурентной сети:}
\begin{itemize}
    \item Обучение без учителя — не требуются размеченные данные
    \item Быстрое обучение
    \item Хорошая интерпретируемость результатов (кластеризация)
    \item Простота реализации
\end{itemize}

\textbf{Недостатки конкурентной сети:}
\begin{itemize}
    \item Требует предварительного знания количества кластеров
    \item Чувствительна к начальной инициализации весов
    \item Может не использовать все нейроны (проблема "мертвых нейронов")
    \item Меньшая точность классификации по сравнению с обучением с учителем
\end{itemize}

\section{Выводы}

\begin{enumerate}
    \item \textbf{Успешная реализация:} Конкурентная нейронная сеть реализована и обучена на 15 образах 5 классов (≤, ≥, ≠, ≈, ≅) размером 6×6.
    
    \item \textbf{Кластеризация:} Сеть успешно разбила образы на 5 кластеров, каждый из которых соответствует одному классу. Похожие образы были спроецированы в один кластер.
    
    \item \textbf{Устойчивость к шуму:} При уровне шума до 20\% — 90\% успешное распознавание. При 30\% шума — 80\% успешность.
    
    \item \textbf{Критический порог:} При 40–50\% шума качество падает до 52–64\%. Это связано с тем, что при высоком уровне шума образы становятся слишком похожими на другие классы.
    
    \item \textbf{Частотно-зависимое обучение:} Использование частотно-зависимого метода позволило избежать проблемы "мертвых нейронов" — все 5 нейронов активно участвуют в кластеризации.
    
    \item \textbf{Сравнение с другими сетями:} Конкурентная сеть хорошо подходит для задач кластеризации без учителя, но уступает сетям с учителем в точности классификации.
    
    \item \textbf{Рекомендации:} Использовать конкурентную сеть при необходимости кластеризации данных без предварительной разметки, когда известно примерное количество кластеров.
\end{enumerate}

\end{document}
