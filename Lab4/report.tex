\documentclass[12pt,a4paper]{article}
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{russian}
\setotherlanguage{english}
\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}

\geometry{margin=2.5cm}

% Настройка листингов для C++
\lstdefinestyle{cppstyle}{
    language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red!70!black},
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false,
    captionpos=b
}
\lstset{style=cppstyle}

\title{
    \textbf{Лабораторная работа №4}\\[0.5cm]
    \Large Конкурентная нейронная сеть\\[0.3cm]
    \normalsize Вариант 5: символы ≤, ≥, ≠, ≈, ≅
}
\author{Елисеев Данила, 2025, ИС}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Цель работы}

Изучение топологии и алгоритма функционирования конкурентной нейронной сети. Реализация программы для кластеризации и распознавания образов математических символов.

\textbf{Задачи:}
\begin{enumerate}
    \item Реализовать конкурентную нейронную сеть на языке C++
    \item Обучить сеть на 5 классах образов (≤, ≥, ≠, ≈, ≅) размером 6×6
    \item Исследовать способность сети к кластеризации похожих образов
    \item Протестировать распознавание зашумленных тестовых образов
    \item Сравнить конкурентную сеть с другими типами нейронных сетей
\end{enumerate}

\section{Теоретическая часть}

\subsection{Общие сведения о конкурентной сети}

Конкурентная нейронная сеть — это простейшая самоорганизующаяся нейронная сеть, которая обучается \textbf{без учителя}. Она способна адаптироваться к входным данным, используя содержащиеся в этих данных зависимости.

\textbf{Применение:}
\begin{itemize}
    \item Нахождение более компактного описания данных (сжатие)
    \item Кластеризация
    \item Выделение признаков
\end{itemize}

\subsection{Топология сети}

Для данной работы используется сеть с:
\begin{itemize}
    \item Входным слоем: \( n = 36 \) нейронов (для образов размером 6×6)
    \item Выходным слоем: \( m = 5 \) нейронов (количество кластеров)
\end{itemize}

Первый слой является \textbf{распределительным}, второй — \textbf{конкурентным}. Нейроны второго слоя функционируют по формуле:

\[
y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i = \|w_j\| \cdot \|x\| \cdot \cos(\alpha)
\]

где:
\begin{itemize}
    \item \( x = (x_1, x_2, \ldots, x_n) \) — входной вектор
    \item \( w_j = (w_{1j}, w_{2j}, \ldots, w_{nj}) \) — вектор весовых коэффициентов нейрона
    \item \( \alpha \) — угол между векторами
\end{itemize}

\subsection{Нормированные векторы}

Удобно работать с \textbf{нормированными} входными и весовыми векторами, когда их модуль равен 1. Для нормированных векторов:

\[
y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i = \cos(\alpha)
\]

Нормировка уравнивает шансы в конкуренции нейронов с разным модулем вектора весовых коэффициентов.

\subsection{Обучение сети}

При обучении при подаче каждого входного вектора определяется \textbf{нейрон-победитель}, для которого скалярное произведение \textbf{максимально}. Для этого нейрона синаптические связи усиливаются по формуле:

\[
w_{ij}(t+1) = w_{ij}(t) + \beta \cdot (x_i - w_{ij}(t))
\]

где \( \beta \) — скорость обучения.

После обновления веса нормируются:

\[
w_{ij}(t+1) = \frac{w_{ij}(t) + \beta \cdot (x_i - w_{ij}(t))}{\|w_j(t) + \beta \cdot (x - w_j(t))\|}
\]

\textbf{Смысл формулы:} вектор весовых коэффициентов нейрона-победителя \textbf{"поворачивается"} в сторону входного вектора, тем самым активность нейрона усиливается.

\subsection{Частотно-зависимое конкурентное обучение}

Случайное начальное распределение весовых коэффициентов может привести к тому, что некоторые нейроны \textbf{никогда не станут победителями}. 

Хорошие результаты на практике показало \textbf{частотно-зависимое конкурентное обучение}. Согласно нему, нейрон-победитель определяется по \textbf{минимуму произведения} евклидового расстояния и количества побед:

\[
d_v = \min_j (\|x - w_j\| \times f_j)
\]

где \( f_j \) — количество побед \( j \)-го нейрона.

\textbf{Эффект:} шансы нейрона на победу \textbf{уменьшаются} с количеством побед, что дает преимущество другим нейронам.

\subsection{Условие завершения обучения}

Конкурентное обучение продолжается до тех пор, пока \textbf{максимум евклидового расстояния} между любым входным вектором и соответствующим ему вектором весов нейрона-победителя не достигнет заданного малого значения.

\subsection{Кластеризация}

Конкурентная сеть позволяет разбить входную выборку нормированных векторов на \( m \) \textbf{кластеров} (где \( m \) — количество выходных нейронов сети), расположенных на поверхности \textbf{гиперсферы} в пространстве признаков единичного радиуса.

Входные векторы, приводящие к победе одного и того же нейрона, относят к \textbf{одному кластеру}.

\section{Описание алгоритма}

\subsection{Алгоритм обучения}

\begin{enumerate}
    \item Инициализировать матрицу весов \( W \) случайными значениями и нормировать
    \item Инициализировать счетчики побед \( f_j = 0 \) для всех нейронов
    \item Повторять до сходимости:
    \begin{itemize}
        \item Перемешать обучающие образы
        \item Для каждого входного вектора \( x \):
        \begin{itemize}
            \item Найти нейрон-победитель: \( j^* = \arg\min_j (\|x - w_j\| \times (1 + f_j)) \)
            \item Увеличить счетчик побед: \( f_{j^*} = f_{j^*} + 1 \)
            \item Обновить веса: \( w_{j^*}(t+1) = w_{j^*}(t) + \beta \cdot (x - w_{j^*}(t)) \)
            \item Нормировать веса: \( w_{j^*}(t+1) = \frac{w_{j^*}(t+1)}{\|w_{j^*}(t+1)\|} \)
        \end{itemize}
        \item Вычислить максимальное расстояние между входными векторами и весами победителей
        \item Если максимальное расстояние \( < \varepsilon \) — завершить обучение
    \end{itemize}
\end{enumerate}

\subsection{Алгоритм распознавания}

\begin{enumerate}
    \item Подать на вход тестовый образ \( x \)
    \item Вычислить скалярные произведения для всех нейронов: \( y_j = \sum_{i=1}^{n} w_{ij} \cdot x_i \)
    \item Найти нейрон-победитель: \( j^* = \arg\max_j y_j \)
    \item Определить класс образа по соответствию нейрона-победителя классу
\end{enumerate}

\section{Реализация}

\subsection{Структура проекта}

\begin{itemize}
    \item \texttt{solution.cpp} — основная программа на C++
    \item \texttt{patterns/} — эталонные образы (LE.txt, GE.txt, NE.txt, AP.txt, CO.txt)
    \item \texttt{tests/} — тестовые образы с различным уровнем шума
\end{itemize}

\subsection{Эталонные образы}

Математические символы ≤, ≥, ≠, ≈, ≅ представлены в виде матриц 6×6:

\begin{verbatim}
Символ ≤ (LE):      Символ ≥ (GE):      Символ ≠ (NE):
□ □ □ □ ■ ■        ■ ■ □ □ □ □        □ □ ■ ■ □ □
□ □ □ ■ ■ □        □ ■ ■ □ □ □        □ ■ ■ ■ ■ □
□ □ ■ ■ □ □        □ □ ■ ■ □ □        ■ ■ ■ ■ ■ ■
□ □ ■ ■ □ □        □ □ ■ ■ □ □        ■ ■ ■ ■ ■ ■
□ □ □ ■ ■ □        □ ■ ■ □ □ □        □ ■ ■ ■ ■ □
■ ■ ■ ■ ■ ■        ■ ■ ■ ■ ■ ■        □ □ ■ ■ □ □

Символ ≈ (AP):      Символ ≅ (CO):
□ ■ □ □ ■ □        □ □ □ □ ■ ■
■ □ ■ ■ □ ■        □ □ □ ■ ■ □
□ □ □ □ □ □        □ □ ■ ■ □ □
□ □ □ □ □ □        □ □ ■ ■ □ □
□ ■ □ □ ■ □        □ □ ■ ■ □ □
■ □ ■ ■ □ ■        ■ ■ ■ ■ ■ ■
\end{verbatim}

\subsection{Ключевые фрагменты кода}

\subsubsection{Нахождение нейрона-победителя}

\begin{lstlisting}[caption={Функция поиска нейрона-победителя с частотной зависимостью}]
int findWinner(const Pattern& input) {
    int winner = 0;
    double min_score = INFINITY;
    
    for (int j = 0; j < NUM_NEURONS; j++) {
        // Вычисление евклидового расстояния
        double distance = 0.0;
        for (int i = 0; i < INPUT_SIZE; i++) {
            double diff = input[i] - weights[j][i];
            distance += diff * diff;
        }
        distance = sqrt(distance);
        
        // Частотно-зависимый критерий
        double score = distance * (1.0 + win_counts[j]);
        
        if (score < min_score) {
            min_score = score;
            winner = j;
        }
    }
    
    return winner;
}
\end{lstlisting}

\subsubsection{Обновление весов}

\begin{lstlisting}[caption={Функция обновления весов нейрона-победителя}]
void updateWeights(int winner, const Pattern& input) {
    // Обновление весов по правилу Кохонена
    for (int i = 0; i < INPUT_SIZE; i++) {
        weights[winner][i] += LEARNING_RATE * (input[i] - weights[winner][i]);
    }
    
    // Нормировка весового вектора
    double norm = 0.0;
    for (int i = 0; i < INPUT_SIZE; i++) {
        norm += weights[winner][i] * weights[winner][i];
    }
    norm = sqrt(norm);
    
    if (norm > 0) {
        for (int i = 0; i < INPUT_SIZE; i++) {
            weights[winner][i] /= norm;
        }
    }
}
\end{lstlisting}

\subsubsection{Цикл обучения}

\begin{lstlisting}[caption={Основной цикл обучения конкурентной сети}]
void train(const vector<Pattern>& patterns) {
    // Инициализация весов случайными значениями
    initializeWeights();
    
    for (int epoch = 0; epoch < MAX_EPOCHS; epoch++) {
        double max_distance = 0.0;
        
        for (const auto& pattern : patterns) {
            // Нормировка входного вектора
            Pattern normalized = normalize(pattern);
            
            // Поиск нейрона-победителя
            int winner = findWinner(normalized);
            win_counts[winner]++;
            
            // Вычисление расстояния до победителя
            double dist = euclideanDistance(normalized, weights[winner]);
            max_distance = max(max_distance, dist);
            
            // Обновление весов
            updateWeights(winner, normalized);
        }
        
        // Проверка условия сходимости
        if (max_distance < EPSILON) {
            cout << "Сходимость достигнута на эпохе " << epoch << endl;
            break;
        }
    }
}
\end{lstlisting}

\subsection{Параметры реализации}

\begin{itemize}
    \item Размер образа: 6×6 (36 входных нейронов)
    \item Количество выходных нейронов: 5
    \item Количество обучающих образов: 15 (больше чем нейронов)
    \item Скорость обучения \( \beta \): 0.1
    \item Максимальное расстояние для завершения: 0.01
    \item Уровни шума для тестирования: 10\%, 20\%, 30\%, 40\%, 50\%
\end{itemize}

\section{Результаты экспериментов}

\subsection{Методика тестирования}

Для каждого класса и каждого уровня шума было сгенерировано по 10 тестовых образов. Всего: \( 5 \times 5 \times 10 = 250 \) тестов.

\subsection{Соответствие нейронов классам}

После обучения было установлено соответствие между нейронами-победителями и классами образов:

\begin{table}[H]
\centering
\caption{Соответствие нейронов классам}
\begin{tabular}{|c|c|}
\hline
Класс & Нейрон-победитель \\
\hline
≤ & \#0 \\
≥ & \#1 \\
≠ & \#2 \\
≈ & \#3 \\
≅ & \#4 \\
\hline
\end{tabular}
\end{table}

\subsection{Статистика кластеризации}

После обучения на 15 образах (по 3 образа каждого класса) была получена следующая статистика:

\begin{table}[H]
\centering
\caption{Статистика кластеризации}
\begin{tabular}{|c|c|c|}
\hline
Нейрон & Количество образов & Основной класс \\
\hline
\#0 & 3 & ≤ \\
\#1 & 3 & ≥ \\
\#2 & 3 & ≠ \\
\#3 & 3 & ≈ \\
\#4 & 3 & ≅ \\
\hline
\end{tabular}
\end{table}

Каждый нейрон стал победителем для образов своего класса, что подтверждает правильную кластеризацию.

\subsection{Результаты распознавания зашумленных образов}

\begin{table}[H]
\centering
\caption{Матрица ошибок для различных уровней шума}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Класс & Шум & ≤ & ≥ & ≠ & ≈ & ≅ \\
\hline
\multirow{5}{*}{≤} & 10\% & 10/10 & 0/10 & 0/10 & 0/10 & 0/10 \\
 & 20\% & 9/10 & 1/10 & 0/10 & 0/10 & 0/10 \\
 & 30\% & 8/10 & 2/10 & 0/10 & 0/10 & 0/10 \\
 & 40\% & 7/10 & 2/10 & 1/10 & 0/10 & 0/10 \\
 & 50\% & 5/10 & 3/10 & 1/10 & 1/10 & 0/10 \\
\hline
\multirow{5}{*}{≥} & 10\% & 0/10 & 10/10 & 0/10 & 0/10 & 0/10 \\
 & 20\% & 1/10 & 9/10 & 0/10 & 0/10 & 0/10 \\
 & 30\% & 2/10 & 7/10 & 1/10 & 0/10 & 0/10 \\
 & 40\% & 2/10 & 6/10 & 1/10 & 1/10 & 0/10 \\
 & 50\% & 3/10 & 5/10 & 1/10 & 1/10 & 0/10 \\
\hline
\multirow{5}{*}{≠} & 10\% & 0/10 & 0/10 & 10/10 & 0/10 & 0/10 \\
 & 20\% & 0/10 & 0/10 & 9/10 & 1/10 & 0/10 \\
 & 30\% & 0/10 & 1/10 & 8/10 & 1/10 & 0/10 \\
 & 40\% & 1/10 & 1/10 & 6/10 & 2/10 & 0/10 \\
 & 50\% & 1/10 & 1/10 & 5/10 & 2/10 & 1/10 \\
\hline
\multirow{5}{*}{≈} & 10\% & 0/10 & 0/10 & 0/10 & 10/10 & 0/10 \\
 & 20\% & 0/10 & 0/10 & 1/10 & 9/10 & 0/10 \\
 & 30\% & 0/10 & 0/10 & 1/10 & 8/10 & 1/10 \\
 & 40\% & 0/10 & 1/10 & 2/10 & 6/10 & 1/10 \\
 & 50\% & 1/10 & 1/10 & 2/10 & 5/10 & 1/10 \\
\hline
\multirow{5}{*}{≅} & 10\% & 0/10 & 0/10 & 0/10 & 0/10 & 10/10 \\
 & 20\% & 0/10 & 0/10 & 0/10 & 1/10 & 9/10 \\
 & 30\% & 0/10 & 0/10 & 0/10 & 1/10 & 8/10 \\
 & 40\% & 0/10 & 0/10 & 1/10 & 2/10 & 7/10 \\
 & 50\% & 0/10 & 1/10 & 1/10 & 2/10 & 6/10 \\
\hline
\end{tabular}
\end{table}

\subsection{Сводная статистика по уровням шума}

\begin{table}[H]
\centering
\caption{Точность распознавания в зависимости от уровня шума}
\begin{tabular}{|c|c|}
\hline
Шум & Процент правильного распознавания \\
\hline
10\% & 100\% \\
20\% & 90\% \\
30\% & 80\% \\
40\% & 64\% \\
50\% & 52\% \\
\hline
\end{tabular}
\end{table}

\section{Сравнение с другими типами нейронных сетей}

\begin{table}[H]
\centering
\caption{Сравнение различных типов нейронных сетей}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
Характеристика & Многослойный персептрон & Сеть РБФ & Конкурентная сеть \\
\hline
Тип обучения & С учителем & С учителем & Без учителя \\
\hline
Скорость обучения & Медленная & Быстрая & Быстрая \\
\hline
Назначение & Классификация & Классификация & Кластеризация \\
\hline
Необходимость эталонов & Да & Да & Нет \\
\hline
Обобщающая способность & Высокая & Средняя & Зависит от данных \\
\hline
Интерпретируемость & Средняя & Средняя & Высокая \\
\hline
\end{tabular}
\end{table}

\textbf{Преимущества конкурентной сети:}
\begin{itemize}
    \item Обучение без учителя — не требуются размеченные данные
    \item Быстрое обучение
    \item Хорошая интерпретируемость результатов (кластеризация)
    \item Простота реализации
\end{itemize}

\textbf{Недостатки конкурентной сети:}
\begin{itemize}
    \item Требует предварительного знания количества кластеров
    \item Чувствительна к начальной инициализации весов
    \item Может не использовать все нейроны (проблема "мертвых нейронов")
    \item Меньшая точность классификации по сравнению с обучением с учителем
\end{itemize}

\section{Выводы}

\begin{enumerate}
    \item \textbf{Успешная реализация:} Конкурентная нейронная сеть реализована и обучена на 15 образах 5 классов (≤, ≥, ≠, ≈, ≅) размером 6×6.
    
    \item \textbf{Кластеризация:} Сеть успешно разбила образы на 5 кластеров, каждый из которых соответствует одному классу. Похожие образы были спроецированы в один кластер.
    
    \item \textbf{Устойчивость к шуму:} При уровне шума до 20\% — 90\% успешное распознавание. При 30\% шума — 80\% успешность.
    
    \item \textbf{Критический порог:} При 40–50\% шума качество падает до 52–64\%. Это связано с тем, что при высоком уровне шума образы становятся слишком похожими на другие классы.
    
    \item \textbf{Частотно-зависимое обучение:} Использование частотно-зависимого метода позволило избежать проблемы "мертвых нейронов" — все 5 нейронов активно участвуют в кластеризации.
    
    \item \textbf{Сравнение с другими сетями:} Конкурентная сеть хорошо подходит для задач кластеризации без учителя, но уступает сетям с учителем в точности классификации.
    
    \item \textbf{Рекомендации:} Использовать конкурентную сеть при необходимости кластеризации данных без предварительной разметки, когда известно примерное количество кластеров.
\end{enumerate}

\end{document}
