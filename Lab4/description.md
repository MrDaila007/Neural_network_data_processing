# Лабораторная работа №4
## Конкурентная нейронная сеть

### 1. Цель работы

Изучение топологии, алгоритма функционирования конкурентной нейронной сети.

### 2. Теоретические сведения

**Самоорганизующиеся нейронные сети** обучаются **без учителя**. Они способны адаптироваться к входным данным, используя содержащиеся в этих данных зависимости. Такие сети используются для:
- Нахождения более компактного описания данных (сжатия)
- Кластеризации
- Выделения признаков

Конкурентная сеть является простейшей самоорганизующейся нейронной сетью.

#### Топология конкурентной сети

```
    Входной слой           Выходной слой
    (распределит.)         (конкурентный)
    
         x₁ ──────────────────┐
          │       w₁ⱼ         │
          │    ┌──────────────┼───────► y₁
          │    │              │         ↑
          │    │              │         │
         x₂ ───┼──────────────┼───────► y₂    "Победитель
          │    │    w₂ⱼ       │         ↑      получает
          │    │              │         │      всё"
         ...  ...            ...       ...      (WTA)
          │    │              │         │
          │    │              │         │
         xₙ ───┴──────────────┴───────► yₘ
                   wₙⱼ                  ↑
                                        │
                              Только один нейрон
                              активен (победитель)
    
    Рис. 5.1. Конкурентная нейронная сеть
```

Первый слой является **распределительным**. Нейроны второго слоя функционируют по формуле:

```
      n
yⱼ = ∑ wᵢⱼ × xᵢ = ‖wⱼ‖ × ‖x‖ × cos(α)
     i=1
```

где:
- `x = (x₁, x₂, ..., xᵢ, ..., xₙ)` – входной вектор
- `wⱼ = (w₁ⱼ, w₂ⱼ, ..., wᵢⱼ, ..., wₙⱼ)` – вектор весовых коэффициентов нейрона
- `‖x‖` и `‖wⱼ‖` – их модули
- `α` – угол между ними

#### Обучение

При обучении нейронной сети при подаче каждого входного вектора определяется **нейрон-победитель**, для которого скалярное произведение (формула выше) **максимально**. Для этого нейрона синаптические связи усиливаются по формуле:

```
wᵢⱼ(t+1) = wᵢⱼ(t) + β × (xᵢ - wᵢⱼ(t))
```

где `β` – скорость обучения.

**Смысл формулы:** вектор весовых коэффициентов нейрона-победителя **"поворачивается"** в сторону входного вектора, тем самым активность нейрона усиливается.

```
                  wⱼ(t+1)
                    ↗
                   ╱
                  ╱  ← Новое положение
                 ╱      вектора весов
                ╱
    ──────────●────────────► x
              │ ╲
              │  ╲ wⱼ(t) ← Старое положение
              │   ╲        вектора весов
              │
              
    Вектор весов "поворачивается" к входному вектору x
```

#### Нормированные векторы

Удобно работать с **нормированными** входными и весовыми векторами, когда их модуль равен 1. Нормировка уравнивает шансы в конкуренции нейронов с разным модулем вектора весовых коэффициентов.

**Для нормированных векторов:**

```
      n
yⱼ = ∑ wᵢⱼ × xᵢ = cos(α)
     i=1
```

**Обновление весов с нормировкой:**

```
              wᵢⱼ(t) + β × (xᵢ - wᵢⱼ(t))
wᵢⱼ(t+1) = ─────────────────────────────────
            ‖wⱼ(t) + β × (x - wⱼ(t))‖
```

#### Частотно-зависимое конкурентное обучение

Случайное начальное распределение весовых коэффициентов может привести к тому, что некоторые нейроны **никогда не станут победителями**, так как их весовые векторы окажутся удаленными от всех входных векторов.

Хорошие результаты на практике показало **частотно-зависимое конкурентное обучение**. Согласно нему, нейрон-победитель определяется по **минимуму произведения** евклидового расстояния и количества побед:

```
dᵥ = min (‖x - wⱼ‖ × fⱼ)
      j
```

где `fⱼ` – количество побед j-го нейрона.

**Эффект:** шансы нейрона на победу **уменьшаются** с количеством побед, что дает преимущество другим нейронам.

#### Условие завершения обучения

Конкурентное обучение продолжается до тех пор, пока **максимум евклидового расстояния** между любым входным вектором и соответствующим ему вектором весов нейрона-победителя не достигнет заданного малого значения.

#### Кластеризация

Конкурентная сеть позволяет разбить входную выборку нормированных векторов на **m кластеров** (где m – количество выходных нейронов сети), расположенных на поверхности **гиперсферы** в пространстве признаков единичного радиуса.

```
           Пространство признаков (2D пример)
           
                      ●● ←─ Кластер 1
                     ●●●    (нейрон 1 - победитель)
                    ●●
                   ╱
                  ╱
    ─────────────●─────────────────────
                 ╲
                  ╲            ○○
                   ╲          ○○○○ ←─ Кластер 2
                    ╲        ○○○     (нейрон 2 - победитель)
                     ╲      ○○
                      ╲
                       ▽▽▽
                      ▽▽▽▽ ←─ Кластер 3
                       ▽▽    (нейрон 3 - победитель)
```

Входные векторы, приводящие к победе одного и того же нейрона, относят к **одному кластеру**.

#### Сеть Кохонена

Кохонен предложил внести в правило конкурентного обучения информацию о **расположении нейронов** в выходном слое. Для этого нейроны упорядочиваются в **одномерные или двухмерные решетки**.

Вводится функция `h(t,k,j)` – **сила влияния** между нейроном-победителем `k` и нейроном `j` в момент времени `t`:

- Для `j = k` эта функция всегда равна 1
- Уменьшается с ростом расстояния между `k` и `j` в решетке
- С течением времени **радиус влияния** обычно сужается

**Обновление весов для всех нейронов:**

```
wᵢⱼ(t+1) = wᵢⱼ(t) + β × h(t,k,j) × (xᵢ - wᵢⱼ(t))
```

#### Функция расстояния h(t,k,j)

В качестве функции `h(t,k,j)` может использоваться:
- **Гауссовый колокол** с параметром σ, зависящим от времени
- Функция вида **"мексиканская шляпа"**

```
    h(t,k,j)
        │
     1.0┤           ╱╲
        │          ╱  ╲ ← Гауссов колокол
        │         ╱    ╲
     0.5┤        ╱      ╲
        │       ╱        ╲
        │      ╱          ╲
     0.0┼─────╱────────────╲─────────►
              k (победитель)        Расстояние
                                    до нейрона j
    
    
    h(t,k,j)
        │
     1.0┤           ╱╲
        │          ╱  ╲
        │         ╱    ╲
     0.5┤        ╱      ╲
        │       ╱        ╲      ← "Мексиканская шляпа"
        │  ────╱          ╲────     (возбуждение в центре,
    -0.2┼─╱──────────────────╲──►   торможение по краям)
              k (победитель)
    
    Рис. 5.2. Примеры функций расстояния в сетях Кохонена
```

#### Результат работы сети Кохонена

В результате модификации конкурентного обучения сеть Кохонена не только **кластеризирует** входные примеры, но и **упорядочивает** их в виде одномерной или двухмерной решетки.

**Преимущества:**
- Позволяет получить дополнительную информацию о **близости кластеров**
- Если два кластера проецируются на **соседние нейроны** решетки, это говорит об их близости в исходном пространстве признаков

**Ограничения:**
- Обратное неверно: из-за уменьшения размерности пространства отношение близости сохраняется только для ограниченного числа кластеров

### 3. Задание

1. Ознакомьтесь с теоретической частью.
2. Написать программу на С, С++, реализующую конкурентную нейронную сеть.
3. Обучить конкурентную сеть с использованием правила (5.5) на количество образов, превышающих количество нейронов сети. Рекомендуется использовать нормированные векторы. Исходные данные - 5 классов образов, размер идеального образа 6×6 (в соответствии с вариантом).
4. Убедиться, что похожие образы были спроецированы сетью в один кластер (подача их на вход активизирует один и тот же нейрон).
5. Подать на вход тестовые образы, отличные от образов из обучающей выборки. Сделать выводы.
6. Напишите отчет.

**Пример вывода результатов:**

```
┌─────────────────────────────────────────────────────────────┐
│ ОБУЧЕНИЕ КОНКУРЕНТНОЙ СЕТИ                                  │
├─────────────────────────────────────────────────────────────┤
│ Класс 1 → Нейрон-победитель: #2                             │
│ Класс 2 → Нейрон-победитель: #4                             │
│ Класс 3 → Нейрон-победитель: #1                             │
│ Класс 4 → Нейрон-победитель: #3                             │
│ Класс 5 → Нейрон-победитель: #5                             │
├─────────────────────────────────────────────────────────────┤
│ ТЕСТИРОВАНИЕ                                                │
├─────────────────────────────────────────────────────────────┤
│ Тестовый образ (зашумленный класс 3):                       │
│   ■ □ ■ ■ □ ■                                               │
│   □ ■ ■ ■ ■ □                                               │
│   ■ ■ □ □ ■ ■                                               │
│   ■ ■ □ □ ■ ■                                               │
│   □ ■ ■ ■ ■ □                                               │
│   ■ □ ■ ■ □ ■                                               │
│                                                             │
│ Результат: Нейрон-победитель #1 (соответствует классу 3) ✓  │
├─────────────────────────────────────────────────────────────┤
│ Статистика кластеризации:                                   │
│   Нейрон #1: 12 образов (класс 3)                           │
│   Нейрон #2: 10 образов (класс 1)                           │
│   Нейрон #3: 11 образов (класс 4)                           │
│   Нейрон #4: 9 образов (класс 2)                            │
│   Нейрон #5: 8 образов (класс 5)                            │
└─────────────────────────────────────────────────────────────┘
```

#### Содержание отчета:

- топология конкурентной нейронной сети;
- основные формулы обучения и воспроизведения;
- идеальные образы для обучения сети;
- тестовые зашумленные образы;
- результаты воспроизведения;
- результаты сравнения конкурентной нейронной сети с сетью РБФ и многослойным персептроном;
- выводы: преимущества и недостатки конкурентной нейронной сети.

#### Сравнение нейронных сетей

| Характеристика | Многослойный персептрон | Сеть РБФ | Конкурентная сеть |
|----------------|-------------------------|----------|-------------------|
| Тип обучения | С учителем | С учителем | Без учителя |
| Скорость обучения | Медленная | Быстрая | Быстрая |
| Назначение | Классификация | Классификация | Кластеризация |
| Необходимость эталонов | Да | Да | Нет |
| Обобщающая способность | Высокая | Средняя | Зависит от данных |

#### Варианты задания

| № варианта | 1-ый класс | 2-ой класс | 3-ий класс | 4-ый класс | 5-ый класс |
|------------|------------|------------|------------|------------|------------|
| 1          | 2          | 3          | 4          | 5          | 7          |
| 2          | N          | F          | I          | P          | D          |
| 3          | ∧          | ∨          | ∃          | ⊂          | ⊃          |
| 4          | ×          | ÷          | ×          | ÷          | ±          |
| 5          | ≤          | ≥          | ≠          | ≈          | ≅          |
| 6          | L          | U          | T          | O          | K          |
| 7          | →          | ←          | ↔          | ⇔          | ⇐          |

### 4. Контрольные вопросы

1. Смысл самообучения.
2. Обучение конкурентной нейронной сети.
3. Определение нейрона-победителя.
4. Сеть Кохонена.
5. Использование самоорганизующихся сетей.
6. Достоинства и недостатки данного типа нейронной сети.
