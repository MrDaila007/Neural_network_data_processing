# Лабораторная работа №4
## Конкурентная нейронная сеть

### 1. Цель работы

Изучение топологии, алгоритма функционирования конкурентной нейронной сети.

### 2. Теоретические сведения

Самоорганизующиеся нейронные сети обучаются без учителя. Они способны адаптироваться к входным данным, используя содержащиеся в этих данных зависимости. Такие сети используются для нахождения более компактного описания данных (сжатия), кластеризации, выделения признаков.

Конкурентная сеть является простейшей самоорганизующейся нейронной сетью.

#### Топология конкурентной сети

Первый слой является распределительным. Нейроны второго слоя функционируют по формуле:

```
y_j = ∑ w_ij * x_i = |w_j| * |x| * cos(α)
      i=1
```

где:
- `x = (x_1, x_2, ..., x_i, ..., x_n)` – входной вектор
- `w_j = (w_1j, w_2j, ..., w_ij, ..., w_nj)` – вектор весовых коэффициентов нейрона
- `|x|` и `|w_j|` – их модули
- `α` – угол между ними

#### Обучение

При обучении нейронной сети при подаче каждого входного вектора определяется нейрон-победитель, для которого формула (5.1) максимальна. Для этого нейрона синаптические связи усиливаются по формуле:

```
w_ij(t+1) = w_ij(t) + β * (x_i - w_ij(t))
```

где `β` – скорость обучения.

Смысл этой формулы в том, что вектор весовых коэффициентов нейрона-победителя "поворачивается" в сторону входного вектора, тем самым активность нейрона усиливается.

#### Нормированные векторы

Удобно работать с нормированными входными и весовыми векторами, когда их модуль равен 1. Нормировка уравнивает шансы в конкуренции нейронов с разным модулем вектора весовых коэффициентов.

Выражение для нормированных векторов:

```
y_j = ∑ w_ij * x_i = cos(α)
      i=1
```

Выражение для обновления весов нормированных векторов:

```
w_ij(t+1) = (w_ij(t) + β * (x_i - w_ij(t))) / ||w_j(t) + β * (x - w_j(t))||
```

#### Частотно-зависимое конкурентное обучение

Случайное начальное распределение весовых коэффициентов может привести к тому, что некоторые нейроны никогда не станут победителями, так как их весовые векторы окажутся удаленными от всех входных векторов. Хорошие результаты на практике показало частотно-зависимое конкурентное обучение.

Согласно нему, нейрон-победитель определяется по минимуму произведения евклидового расстояния между входным и весовым вектором и количеством побед данного нейрона `f_j`:

```
d_v = min_j (||x - w_j|| * f_j)
```

Шансы нейрона на победу уменьшаются с количеством побед, что дает преимущество другим нейронам.

#### Условие завершения обучения

Конкурентное обучение продолжается до тех пор, пока максимум евклидового расстояния между любым входным вектором и соответствующим ему вектором весов нейрона-победителя не достигнет заданного малого значения.

#### Кластеризация

Конкурентная сеть позволяет разбить входную выборку нормированных векторов на `m` (количество выходных нейронов сети) кластеров, расположенных на поверхности гиперсферы в пространстве признаков единичного радиуса. Входные векторы, приводящие к победе одного и того же нейрона, относят к одному кластеру.

#### Сеть Кохонена

Кохонен предложил внести в правило конкурентного обучения информацию о расположении нейронов в выходном слое. Для этого нейроны упорядочиваются в одномерные или двухмерные решетки. Вводится функция, корректирующая изменение весов в зависимости от расстояния до нейрона-победителя `h(t,k,j)` – сила влияния между нейроном-победителем `k` и нейроном `j` в момент времени `t`.

Для `j=k` эта функция всегда равна 1 и уменьшается с ростом расстояния между `k` и `j` в решетке. С течением времени радиус влияния обычно сужается. С использованием этой функции веса меняются для всех нейронов сети, а не только для нейрона-победителя:

```
w_ij(t+1) = w_ij(t) + β * h(t,k,j) * (x_i - w_ij(t))
```

В качестве функции `h(t,k,j)` может использоваться гауссовый колокол с параметром `σ`, зависящим от времени, или функция вида "мексиканская шляпа".

В результате модификации конкурентного обучения сеть Кохонена не только кластеризирует входные примеры, но и упорядочивает их в виде одномерной или двухмерной решетки. Это позволяет получить дополнительную информацию о близости кластеров. Если два кластера проецируются на соседние нейроны решетки, это говорит об их близости в исходном пространстве признаков. Обратное неверно. Из-за уменьшения размерности пространства отношение близости сохраняется только для ограниченного числа кластеров.

### 3. Задание

1. Ознакомьтесь с теоретической частью.
2. Написать программу на С, С++, реализующую конкурентную нейронную сеть.
3. Обучить конкурентную сеть с использованием правила (5.5) на количество образов, превышающих количество нейронов сети. Рекомендуется использовать нормированные векторы. Исходные данные - 5 классов образов, размер идеального образа 6×6 (в соответствии с вариантом).
4. Убедиться, что похожие образы были спроецированы сетью в один кластер (подача их на вход активизирует один и тот же нейрон).
5. Подать на вход тестовые образы, отличные от образов из обучающей выборки. Сделать выводы.
6. Напишите отчет.

#### Содержание отчета:

- топология конкурентной нейронной сети;
- основные формулы обучения и воспроизведения;
- идеальные образы для обучения сети;
- тестовые зашумленные образы;
- результаты воспроизведения;
- результаты сравнения конкурентной нейронной сети с сетью РБФ и многослойным персептроном;
- выводы: преимущества и недостатки конкурентной нейронной сети.

#### Варианты задания

| № варианта | 1-ый класс | 2-ой класс | 3-ий класс | 4-ый класс | 5-ый класс |
|------------|------------|------------|------------|------------|------------|
| 1          | 2          | 3          | 4          | 5          | 7          |
| 2          | N          | F          | I          | P          | D          |
| 3          | ∧          | ∨          | ∃          | ⊂          | ⊃          |
| 4          | ×          | ÷          | ×          | ÷          | ±          |
| 5          | ≤          | ≥          | ≠          | ≈          | ≅          |
| 6          | L          | U          | T          | O          | K          |
| 7          | →          | ←          | ↔          | ⇔          | ⇐          |

### 4. Контрольные вопросы

1. Смысл самообучения.
2. Обучение конкурентной нейронной сети.
3. Определение нейрона-победителя.
4. Сеть Кохонена.
5. Использование самоорганизующихся сетей.
6. Достоинства и недостатки данного типа нейронной сети.
